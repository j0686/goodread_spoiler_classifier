{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader,random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from typing import *\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from predeal_dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_DIM = 50\n",
    "\n",
    "EPOCH = 100\n",
    "BATCH_SIZE = 10 \n",
    "LR = 0.002\n",
    "L2 = 0.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_text_to_file(text:str)->None:\n",
    "    with open(\"./output/log.txt\",\"a\") as fout:\n",
    "        fout.write(text+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [00:10<00:00, 38963.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 lines are discarded\n"
     ]
    }
   ],
   "source": [
    "def load_GLoVe_embedding(dim:int=50)->Dict[str,List[float]]:\n",
    "    res = dict()\n",
    "    error_cnt = 0\n",
    "    with open(f\"./glove.6B/glove.6B.{dim}d.txt\",encoding=\"utf-8\") as fin:\n",
    "        lines = fin.readlines()\n",
    "        for line in tqdm(lines):\n",
    "            elements = line.split()\n",
    "            if len(elements)!=dim+1:\n",
    "                error_cnt+=1\n",
    "                continue\n",
    "            word = elements[0]\n",
    "            vector = list(map(float,elements[1:]))\n",
    "            res[word]=vector\n",
    "        print(f\"{error_cnt} lines are discarded\")\n",
    "    return res\n",
    "\n",
    "glove_emb_50d = load_GLoVe_embedding(EMB_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = string.punctuation\n",
    "stemmer = PorterStemmer()\n",
    "def tokenize(sentence:str,use_stemmer:bool=True)->List[str]:\n",
    "    # sentence = sentence.replace(\"(\",\"( \").replace(\"[\",\"[ \").replace(\"{\",\"{ \")\n",
    "    for p in puncts:\n",
    "        sentence = sentence.replace(p,p+\" \")\n",
    "    res = list()\n",
    "    tmp = sentence.split()\n",
    "    for word in tmp:\n",
    "        if len(word)==0:\n",
    "            continue\n",
    "        if word==\"...\":\n",
    "            res.append(word)\n",
    "            continue        \n",
    "        if word[-1] in puncts:\n",
    "            p = word[-1]\n",
    "            word = word[:-1]\n",
    "            if len(word)>0:\n",
    "                word = word.lower()\n",
    "                if use_stemmer:\n",
    "                    word = stemmer.stem(word)\n",
    "                res.append(word)\n",
    "            res.append(p)\n",
    "        else:\n",
    "            word = word.lower()\n",
    "            if use_stemmer:\n",
    "                word = stemmer.stem(word)\n",
    "            res.append(word)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "spoiler_dataset = sample_sub_spoiler_set(SUBSET_SENTENCE_CNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:05<00:00, 8509.67it/s]\n"
     ]
    }
   ],
   "source": [
    "word_cnt = defaultdict(int)\n",
    "\n",
    "for datum in tqdm(spoiler_dataset):\n",
    "    sentence = datum['review_sentence']\n",
    "    for word in tokenize(sentence,False):\n",
    "        word_cnt[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICTIONARY_SIZE = 5000\n",
    "SEPERATOR_SIGN = \"seperatorsign\"\n",
    "UNKNOWN_WORD = \"unknownword\"\n",
    "\n",
    "word_with_freq = list(word_cnt.items())\n",
    "word_with_freq.sort(key=lambda tup:tup[1],reverse=True)\n",
    "dictionary = word_with_freq[:]\n",
    "dictionary = list(map(lambda tup:tup[0],dictionary))\n",
    "dictionary = list(filter(lambda word:word in glove_emb_50d,dictionary))\n",
    "dictionary = dictionary[:DICTIONARY_SIZE-2]\n",
    "dictionary.append(SEPERATOR_SIGN)\n",
    "dictionary.append(UNKNOWN_WORD)\n",
    "\n",
    "word2id = {word:i for i,word in enumerate(dictionary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = list(glove_emb_50d[word] for word in dictionary[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(vector:List[float])->float:\n",
    "    sq_sum = 0.0\n",
    "    for x in vector:\n",
    "        sq_sum+=x**2\n",
    "    return sq_sum**0.5\n",
    "\n",
    "emb_avg_norm = np.average([norm(v) for v in embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_random_emb():    \n",
    "    res = np.random.randn(EMB_DIM)\n",
    "    return (emb_avg_norm/norm(res))*res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.append(gen_random_emb())\n",
    "embedding.append(gen_random_emb())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JipingZhang\\AppData\\Local\\Temp\\ipykernel_18232\\3915031918.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  embedding_tensor = torch.FloatTensor(embedding)\n"
     ]
    }
   ],
   "source": [
    "embedding_tensor = torch.FloatTensor(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_id = word2id[UNKNOWN_WORD]\n",
    "\n",
    "def parse_id_sequence(sentence):\n",
    "    res = list()\n",
    "    words = tokenize(sentence,False)\n",
    "    for word in words:\n",
    "        if word not in word2id:\n",
    "            res.append(unk_id)\n",
    "        else:\n",
    "            res.append(word2id[word])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMClassifierWithPretrainedEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim:int=2*EMB_DIM, output_dim:int=2, n_layers:int=2):\n",
    "        super().__init__()\n",
    "        embedding_dim = embedding_matrix.size(1)\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, \n",
    "                            bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2+1, output_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        l = len(embedded)\n",
    "        l_tensor = torch.FloatTensor([l]).to(GPU)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        outputs = outputs[-1,:]\n",
    "        outputs_with_l = torch.concat((outputs,l_tensor))\n",
    "        dense_outputs = self.fc(outputs_with_l)\n",
    "        return dense_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTMClassifierWithPretrainedEmbedding(embedding_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0126, -0.0409], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.7203, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "input_ids = parse_id_sequence(\"can can need new new\")\n",
    "input_tensor = torch.LongTensor(input_ids)\n",
    "input_tensor = input_tensor.reshape((-1,1))\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "output = model(input_tensor)\n",
    "print(output)\n",
    "loss_func(output,torch.LongTensor([1])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListDataset(Dataset):\n",
    "    def __init__(self,*lists) -> None:\n",
    "        super().__init__()\n",
    "        if len(lists)==0:\n",
    "            raise ValueError(\"Expecting at least one list\")\n",
    "        l = len(lists[0])\n",
    "        for i,li in enumerate(lists):\n",
    "            if not isinstance(li,(list,tuple,np.ndarray,torch.Tensor)):\n",
    "                raise ValueError(f\"expecting input to be list,tuple,numpy-array or torch's tensor, actually get {type(li)} at {i}-th argument\")\n",
    "            if len(li)!=l:\n",
    "                raise ValueError(f\"length of {i}-th argument is {len(li)}, length of 0-th argument is {l}, they don't match\")\n",
    "        self.lists = lists\n",
    "        self.l = l\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.l\n",
    "    \n",
    "    def __getitem__(self, index) -> Any:\n",
    "        return tuple(map(lambda l:l[index],self.lists))\n",
    "\n",
    "def get_spoiler_dataset(spoiler_dataset_raw:List[dict])->Dataset:\n",
    "    xs = list()\n",
    "    ys = list()\n",
    "    for datum in spoiler_dataset_raw:\n",
    "        xs.append(datum['review_sentence'])\n",
    "        ys.append(datum['label'])\n",
    "    return ListDataset(xs,ys)\n",
    "\n",
    "spoiler_dataset_processed = get_spoiler_dataset(spoiler_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset, test_dataset = random_split(spoiler_dataset_processed, [TRAIN_SET_CNT, VALID_SET_CNT, TEST_SET_CNT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_info(y_actual,y_predict):\n",
    "    y_actual = np.array(y_actual)\n",
    "    y_predict = np.array(y_predict)\n",
    "    y_actual = y_actual.reshape((-1,))\n",
    "    y_predict = y_predict.reshape((-1,))\n",
    "    TP = np.sum((y_actual == 1) & (y_predict == 1))\n",
    "    FP = np.sum((y_actual == 0) & (y_predict == 1))\n",
    "    TN = np.sum((y_actual == 0) & (y_predict == 0))\n",
    "    FN = np.sum((y_actual == 1) & (y_predict == 0))\n",
    "    TPR = TP / (TP + FN)\n",
    "    FPR = FP / (FP + TN)\n",
    "    TNR = TN / (TN + FP)\n",
    "    FNR = FN / (TP + FN)\n",
    "    BER = 1 - (0.5 * (TPR + TNR))\n",
    "    accu = np.sum(y_actual==y_predict)/len(y_actual)\n",
    "    return accu,TP,FP,TN,FN,TPR, FPR, TNR, FNR, BER\n",
    "\n",
    "e = 2.718281828\n",
    "\n",
    "def get_best_ths_with_ber(pred_prop_with_label): \n",
    "    pred_prop_with_label.sort(reverse=True)\n",
    "    valid_set_pos_cnt = sum(tup[1] for tup in pred_prop_with_label)\n",
    "    valid_set_neg_cnt = len(pred_prop_with_label)-valid_set_pos_cnt\n",
    "    best_ths = 1.0\n",
    "    best_ber = 0.5\n",
    "    curr_false_positive = 0\n",
    "    curr_false_negative = valid_set_pos_cnt\n",
    "    for (prob,label) in pred_prop_with_label:\n",
    "        ths = prob-0.00001\n",
    "        if label==1:\n",
    "            curr_false_negative-=1\n",
    "        else:\n",
    "            curr_false_positive+=1\n",
    "        ber = 0.5*(curr_false_negative/valid_set_pos_cnt+curr_false_positive/valid_set_neg_cnt)\n",
    "        if ber<best_ber:\n",
    "            best_ber = ber\n",
    "            best_ths = ths\n",
    "    return best_ths,best_ber\n",
    "\n",
    "def evaluate_dynamic_prob_ths(model,dataset):\n",
    "    model.to(GPU)\n",
    "    e = 2.718281828\n",
    "    y_pred_logits = list()\n",
    "    y_label = list()\n",
    "    with torch.no_grad():\n",
    "        for b_x,b_y in tqdm(dataset):\n",
    "            input_ids = torch.LongTensor(parse_id_sequence(b_x)).to(GPU)\n",
    "            output = model(input_ids).to(\"cpu\").detach()\n",
    "            y_label.append(b_y)\n",
    "            y_pred_logits.append(output.numpy().tolist())\n",
    "    y_pos_prob_pred = list(map(lambda logits:e**logits[1]/(e**logits[0]+e**logits[1]),y_pred_logits))\n",
    "    pred_prob_with_label = list(zip(y_pos_prob_pred,y_label))\n",
    "    judging_ths,ber = get_best_ths_with_ber(pred_prob_with_label)\n",
    "    y_pred = list(int(p>judging_ths) for p in y_pos_prob_pred)\n",
    "    return *get_performance_info(y_label,y_pred),judging_ths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:19<00:00, 261.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8102,\n",
       " 51,\n",
       " 696,\n",
       " 4000,\n",
       " 253,\n",
       " 0.16776315789473684,\n",
       " 0.14821124361158433,\n",
       " 0.8517887563884157,\n",
       " 0.8322368421052632,\n",
       " 0.4902240428584237,\n",
       " 0.49586474531550173)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BiLSTMClassifierWithPretrainedEmbedding(embedding_tensor)\n",
    "\n",
    "# model.fc\n",
    "\n",
    "evaluate_dynamic_prob_ths(model,valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShuffledDataset(Dataset):\n",
    "    def __init__(self,dataset) -> None:\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        indices = list(range(len(dataset)))\n",
    "        random.shuffle(indices)\n",
    "        self.indices = indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index) -> Any:\n",
    "        return self.dataset[self.indices[index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [03:02<00:00, 219.35it/s]\n",
      "100%|██████████| 5000/5000 [00:19<00:00, 257.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ber after epoch 1: 0.36216152604680363\n",
      "       accu    ber    tpr    fpr    tnr    fnr    ths\n",
      "     0.6260 0.3622 0.6513 0.3756 0.6244 0.3487 0.0438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [03:03<00:00, 217.54it/s]\n",
      "100%|██████████| 5000/5000 [00:19<00:00, 256.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ber after epoch 2: 0.3543161032905945\n",
      "       accu    ber    tpr    fpr    tnr    fnr    ths\n",
      "     0.5136 0.3543 0.7961 0.5047 0.4953 0.2039 0.0510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [03:03<00:00, 217.88it/s]\n",
      "100%|██████████| 5000/5000 [00:19<00:00, 254.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ber after epoch 3: 0.36173563166860934\n",
      "       accu    ber    tpr    fpr    tnr    fnr    ths\n",
      "     0.6268 0.3617 0.6513 0.3748 0.6252 0.3487 0.0634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [03:01<00:00, 220.54it/s]\n",
      "100%|██████████| 5000/5000 [00:19<00:00, 258.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ber after epoch 4: 0.36184210526315796\n",
      "       accu    ber    tpr    fpr    tnr    fnr    ths\n",
      "     0.6266 0.3618 0.6513 0.3750 0.6250 0.3487 0.0377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [03:01<00:00, 220.15it/s]\n",
      "100%|██████████| 5000/5000 [00:19<00:00, 257.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ber after epoch 5: 0.36198500403478884\n",
      "       accu    ber    tpr    fpr    tnr    fnr    ths\n",
      "     0.5252 0.3620 0.7664 0.4904 0.5096 0.2336 0.0269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [03:01<00:00, 220.97it/s]\n",
      "100%|██████████| 5000/5000 [00:19<00:00, 259.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ber after epoch 6: 0.36129852954362063\n",
      "       accu    ber    tpr    fpr    tnr    fnr    ths\n",
      "     0.6334 0.3613 0.6447 0.3673 0.6327 0.3553 0.0476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [02:58<00:00, 224.58it/s]\n",
      "100%|██████████| 5000/5000 [00:19<00:00, 251.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ber after epoch 7: 0.360020846409038\n",
      "       accu    ber    tpr    fpr    tnr    fnr    ths\n",
      "     0.6358 0.3600 0.6447 0.3648 0.6352 0.3553 0.0545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [02:57<00:00, 224.90it/s]\n",
      "100%|██████████| 5000/5000 [00:19<00:00, 260.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ber after epoch 8: 0.359973213485161\n",
      "       accu    ber    tpr    fpr    tnr    fnr    ths\n",
      "     0.6330 0.3600 0.6480 0.3680 0.6320 0.3520 0.0712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [03:03<00:00, 217.48it/s]\n",
      "100%|██████████| 5000/5000 [00:19<00:00, 251.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ber after epoch 9: 0.3594408455124182\n",
      "       accu    ber    tpr    fpr    tnr    fnr    ths\n",
      "     0.6340 0.3594 0.6480 0.3669 0.6331 0.3520 0.0424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [03:01<00:00, 220.15it/s]\n",
      "100%|██████████| 5000/5000 [00:19<00:00, 256.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ber after epoch 10: 0.360233793598135\n",
      "       accu    ber    tpr    fpr    tnr    fnr    ths\n",
      "     0.6354 0.3602 0.6447 0.3652 0.6348 0.3553 0.0628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [03:01<00:00, 220.86it/s]\n",
      "100%|██████████| 5000/5000 [00:19<00:00, 256.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ber after epoch 11: 0.3606120550524523\n",
      "       accu    ber    tpr    fpr    tnr    fnr    ths\n",
      "     0.6318 0.3606 0.6480 0.3693 0.6307 0.3520 0.1554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [03:01<00:00, 220.90it/s]\n",
      "100%|██████████| 5000/5000 [00:19<00:00, 254.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ber after epoch 12: 0.359227898323321\n",
      "       accu    ber    tpr    fpr    tnr    fnr    ths\n",
      "     0.6344 0.3592 0.6480 0.3665 0.6335 0.3520 0.0350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [03:00<00:00, 222.19it/s]\n",
      "100%|██████████| 5000/5000 [00:18<00:00, 263.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ber after epoch 13: 0.36034026719268364\n",
      "       accu    ber    tpr    fpr    tnr    fnr    ths\n",
      "     0.6352 0.3603 0.6447 0.3654 0.6346 0.3553 0.0956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [02:56<00:00, 227.24it/s]\n",
      "100%|██████████| 5000/5000 [00:19<00:00, 262.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ber after epoch 14: 0.3607185286470008\n",
      "       accu    ber    tpr    fpr    tnr    fnr    ths\n",
      "     0.6316 0.3607 0.6480 0.3695 0.6305 0.3520 0.0597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [02:55<00:00, 227.67it/s]\n",
      "100%|██████████| 5000/5000 [00:18<00:00, 264.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ber after epoch 15: 0.3607661615708777\n",
      "       accu    ber    tpr    fpr    tnr    fnr    ths\n",
      "     0.6344 0.3608 0.6447 0.3663 0.6337 0.3553 0.1100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [02:55<00:00, 227.67it/s]\n",
      "100%|██████████| 5000/5000 [00:19<00:00, 261.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ber after epoch 16: 0.3596425849547207\n",
      "       accu    ber    tpr    fpr    tnr    fnr    ths\n",
      "     0.6394 0.3596 0.6414 0.3607 0.6393 0.3586 0.0731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [02:56<00:00, 227.05it/s]\n",
      "100%|██████████| 5000/5000 [00:19<00:00, 262.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ber after epoch 17: 0.35843214830090553\n",
      "       accu    ber    tpr    fpr    tnr    fnr    ths\n",
      "     0.4972 0.3584 0.8059 0.5228 0.4772 0.1941 0.0440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [02:55<00:00, 227.81it/s]\n",
      "100%|██████████| 5000/5000 [00:19<00:00, 262.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ber after epoch 18: 0.359743454675872\n",
      "       accu    ber    tpr    fpr    tnr    fnr    ths\n",
      "     0.4774 0.3597 0.8257 0.5451 0.4549 0.1743 0.0582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [02:56<00:00, 226.42it/s]\n",
      "100%|██████████| 5000/5000 [00:19<00:00, 262.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ber after epoch 19: 0.3600796870797095\n",
      "       accu    ber    tpr    fpr    tnr    fnr    ths\n",
      "     0.6328 0.3601 0.6480 0.3682 0.6318 0.3520 0.0870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [02:56<00:00, 226.05it/s]\n",
      "100%|██████████| 5000/5000 [00:19<00:00, 259.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ber after epoch 20: 0.3605728279386712\n",
      "       accu    ber    tpr    fpr    tnr    fnr    ths\n",
      "     0.4874 0.3606 0.8125 0.5336 0.4664 0.1875 0.0375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [02:56<00:00, 227.08it/s]\n",
      "100%|██████████| 5000/5000 [00:19<00:00, 262.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ber after epoch 21: 0.35529678113512064\n",
      "       accu    ber    tpr    fpr    tnr    fnr    ths\n",
      "     0.5002 0.3553 0.8092 0.5198 0.4802 0.1908 0.0352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [02:57<00:00, 225.50it/s]\n",
      "100%|██████████| 5000/5000 [00:19<00:00, 261.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ber after epoch 22: 0.35957814041065184\n",
      "       accu    ber    tpr    fpr    tnr    fnr    ths\n",
      "     0.4806 0.3596 0.8224 0.5415 0.4585 0.1776 0.0317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [02:56<00:00, 226.78it/s]\n",
      "100%|██████████| 5000/5000 [00:19<00:00, 262.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ber after epoch 23: 0.35747108401327\n",
      "       accu    ber    tpr    fpr    tnr    fnr    ths\n",
      "     0.5828 0.3575 0.7105 0.4255 0.5745 0.2895 0.0747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [02:56<00:00, 227.00it/s]\n",
      "100%|██████████| 5000/5000 [00:18<00:00, 264.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ber after epoch 24: 0.3474457545055142\n",
      "       accu    ber    tpr    fpr    tnr    fnr    ths\n",
      "     0.5554 0.3474 0.7632 0.4580 0.5420 0.2368 0.0269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [02:56<00:00, 227.04it/s]\n",
      "100%|██████████| 5000/5000 [00:19<00:00, 260.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ber after epoch 25: 0.35121155742849464\n",
      "       accu    ber    tpr    fpr    tnr    fnr    ths\n",
      "     0.5830 0.3512 0.7237 0.4261 0.5739 0.2763 0.0472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 2223/40000 [00:10<03:03, 205.37it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\JipingZhang\\Desktop\\cse258_assignment2\\play_GLoVe_lstm.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JipingZhang/Desktop/cse258_assignment2/play_GLoVe_lstm.ipynb#X42sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m log_text_to_file(\u001b[39m\"\u001b[39m\u001b[39mstart training bilstm model , dataset is review_text only\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JipingZhang/Desktop/cse258_assignment2/play_GLoVe_lstm.ipynb#X42sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCH):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/JipingZhang/Desktop/cse258_assignment2/play_GLoVe_lstm.ipynb#X42sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mfor\u001b[39;00m step,(b_x,b_y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(ShuffledDataset(train_dataset))):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JipingZhang/Desktop/cse258_assignment2/play_GLoVe_lstm.ipynb#X42sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         input_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mLongTensor(parse_id_sequence(b_x))\u001b[39m.\u001b[39mto(GPU)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JipingZhang/Desktop/cse258_assignment2/play_GLoVe_lstm.ipynb#X42sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         output \u001b[39m=\u001b[39m model(input_ids)\n",
      "File \u001b[1;32mc:\\Users\\JipingZhang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\std.py:1192\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1190\u001b[0m dt \u001b[39m=\u001b[39m cur_t \u001b[39m-\u001b[39m last_print_t\n\u001b[0;32m   1191\u001b[0m \u001b[39mif\u001b[39;00m dt \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m mininterval \u001b[39mand\u001b[39;00m cur_t \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m min_start_t:\n\u001b[1;32m-> 1192\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate(n \u001b[39m-\u001b[39;49m last_print_n)\n\u001b[0;32m   1193\u001b[0m     last_print_n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_print_n\n\u001b[0;32m   1194\u001b[0m     last_print_t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_print_t\n",
      "File \u001b[1;32mc:\\Users\\JipingZhang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\std.py:1243\u001b[0m, in \u001b[0;36mtqdm.update\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1241\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ema_dn(dn)\n\u001b[0;32m   1242\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ema_dt(dt)\n\u001b[1;32m-> 1243\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrefresh(lock_args\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlock_args)\n\u001b[0;32m   1244\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdynamic_miniters:\n\u001b[0;32m   1245\u001b[0m     \u001b[39m# If no `miniters` was specified, adjust automatically to the\u001b[39;00m\n\u001b[0;32m   1246\u001b[0m     \u001b[39m# maximum iteration rate seen so far between two prints.\u001b[39;00m\n\u001b[0;32m   1247\u001b[0m     \u001b[39m# e.g.: After running `tqdm.update(5)`, subsequent\u001b[39;00m\n\u001b[0;32m   1248\u001b[0m     \u001b[39m# calls to `tqdm.update()` will only cause an update after\u001b[39;00m\n\u001b[0;32m   1249\u001b[0m     \u001b[39m# at least 5 more iterations.\u001b[39;00m\n\u001b[0;32m   1250\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxinterval \u001b[39mand\u001b[39;00m dt \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxinterval:\n",
      "File \u001b[1;32mc:\\Users\\JipingZhang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\std.py:1348\u001b[0m, in \u001b[0;36mtqdm.refresh\u001b[1;34m(self, nolock, lock_args)\u001b[0m\n\u001b[0;32m   1346\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1347\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39macquire()\n\u001b[1;32m-> 1348\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdisplay()\n\u001b[0;32m   1349\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m nolock:\n\u001b[0;32m   1350\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\JipingZhang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\std.py:1496\u001b[0m, in \u001b[0;36mtqdm.display\u001b[1;34m(self, msg, pos)\u001b[0m\n\u001b[0;32m   1494\u001b[0m \u001b[39mif\u001b[39;00m pos:\n\u001b[0;32m   1495\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmoveto(pos)\n\u001b[1;32m-> 1496\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msp(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__str__\u001b[39;49m() \u001b[39mif\u001b[39;49;00m msg \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m msg)\n\u001b[0;32m   1497\u001b[0m \u001b[39mif\u001b[39;00m pos:\n\u001b[0;32m   1498\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmoveto(\u001b[39m-\u001b[39mpos)\n",
      "File \u001b[1;32mc:\\Users\\JipingZhang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\std.py:462\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.print_status\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprint_status\u001b[39m(s):\n\u001b[0;32m    461\u001b[0m     len_s \u001b[39m=\u001b[39m disp_len(s)\n\u001b[1;32m--> 462\u001b[0m     fp_write(\u001b[39m'\u001b[39;49m\u001b[39m\\r\u001b[39;49;00m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m s \u001b[39m+\u001b[39;49m (\u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39mmax\u001b[39;49m(last_len[\u001b[39m0\u001b[39;49m] \u001b[39m-\u001b[39;49m len_s, \u001b[39m0\u001b[39;49m)))\n\u001b[0;32m    463\u001b[0m     last_len[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m len_s\n",
      "File \u001b[1;32mc:\\Users\\JipingZhang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\std.py:456\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.fp_write\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfp_write\u001b[39m(s):\n\u001b[0;32m    455\u001b[0m     fp\u001b[39m.\u001b[39mwrite(\u001b[39mstr\u001b[39m(s))\n\u001b[1;32m--> 456\u001b[0m     fp_flush()\n",
      "File \u001b[1;32mc:\\Users\\JipingZhang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\utils.py:195\u001b[0m, in \u001b[0;36mDisableOnWriteError.disable_on_exception.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    194\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 195\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    196\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    197\u001b[0m         \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39merrno \u001b[39m!=\u001b[39m \u001b[39m5\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\iostream.py:575\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"trigger actual zmq send\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \n\u001b[0;32m    566\u001b[0m \u001b[39msend will happen in the background thread\u001b[39;00m\n\u001b[0;32m    567\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    569\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpub_thread\n\u001b[0;32m    570\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpub_thread\u001b[39m.\u001b[39mthread \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    573\u001b[0m ):\n\u001b[0;32m    574\u001b[0m     \u001b[39m# request flush on the background thread\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpub_thread\u001b[39m.\u001b[39;49mschedule(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flush)\n\u001b[0;32m    576\u001b[0m     \u001b[39m# wait for flush to actually get through, if we can.\u001b[39;00m\n\u001b[0;32m    577\u001b[0m     evt \u001b[39m=\u001b[39m threading\u001b[39m.\u001b[39mEvent()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\iostream.py:267\u001b[0m, in \u001b[0;36mIOPubThread.schedule\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_events\u001b[39m.\u001b[39mappend(f)\n\u001b[0;32m    266\u001b[0m     \u001b[39m# wake event thread (message content is ignored)\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event_pipe\u001b[39m.\u001b[39;49msend(\u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    268\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    269\u001b[0m     f()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\zmq\\sugar\\socket.py:696\u001b[0m, in \u001b[0;36mSocket.send\u001b[1;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[0;32m    689\u001b[0m         data \u001b[39m=\u001b[39m zmq\u001b[39m.\u001b[39mFrame(\n\u001b[0;32m    690\u001b[0m             data,\n\u001b[0;32m    691\u001b[0m             track\u001b[39m=\u001b[39mtrack,\n\u001b[0;32m    692\u001b[0m             copy\u001b[39m=\u001b[39mcopy \u001b[39mor\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    693\u001b[0m             copy_threshold\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy_threshold,\n\u001b[0;32m    694\u001b[0m         )\n\u001b[0;32m    695\u001b[0m     data\u001b[39m.\u001b[39mgroup \u001b[39m=\u001b[39m group\n\u001b[1;32m--> 696\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49msend(data, flags\u001b[39m=\u001b[39;49mflags, copy\u001b[39m=\u001b[39;49mcopy, track\u001b[39m=\u001b[39;49mtrack)\n",
      "File \u001b[1;32mzmq\\\\backend\\\\cython\\\\socket.pyx:742\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mzmq\\\\backend\\\\cython\\\\socket.pyx:789\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mzmq\\\\backend\\\\cython\\\\socket.pyx:250\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\zmq\\backend\\cython\\checkrc.pxd:13\u001b[0m, in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = BiLSTMClassifierWithPretrainedEmbedding(embedding_tensor)\n",
    "model.to(GPU)\n",
    "\n",
    "pos_cnt = sum(y for (x,y) in train_dataset)\n",
    "neg_cnt = len(train_dataset)-pos_cnt\n",
    "weights = [pos_cnt/len(train_dataset),neg_cnt/len(train_dataset)]\n",
    "weights = torch.tensor(weights, dtype=torch.float).to(GPU)\n",
    "loss_func = torch.nn.CrossEntropyLoss(weights)\n",
    "weight_params = [param for name, param in model.named_parameters() if 'weight' in name]\n",
    "bias_params = [param for name, param in model.named_parameters() if 'bias' in name]\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': weight_params, 'weight_decay': L2},\n",
    "    {'params': bias_params, 'weight_decay': 0.0}  \n",
    "], lr=LR/BATCH_SIZE) \n",
    "# optimizer = torch.optim.Adam(model.parameters(),LR/BATCH_SIZE,weight_decay=L2)\n",
    "\n",
    "# data_loader = DataLoader(train_dataset,batch_size=1,shuffle=True)\n",
    "log_text_to_file(\"start training bilstm model , dataset is review_text only\")\n",
    "for e in range(EPOCH):\n",
    "    for step,(b_x,b_y) in enumerate(tqdm(ShuffledDataset(train_dataset))):\n",
    "        input_ids = torch.LongTensor(parse_id_sequence(b_x)).to(GPU)\n",
    "        output = model(input_ids)\n",
    "        b_y = torch.LongTensor([b_y])[0].to(GPU)\n",
    "        loss = loss_func(output,b_y)\n",
    "        if (step+1)%BATCH_SIZE==0:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    accu,TP,FP,TN,FN,TPR, FPR, TNR, FNR, BER,ths = evaluate_dynamic_prob_ths(model,valid_dataset)\n",
    "    print(f\"ber after epoch {e+1}: {BER}\")\n",
    "    msg = \"    %7s%7s%7s%7s%7s%7s%7s\\n     %.4f %.4f %.4f %.4f %.4f %.4f %.4f\"%(\"accu\",\"ber\",\"tpr\",\"fpr\",\"tnr\",\"fnr\",\"ths\",accu,BER,TPR,FPR,TNR,FNR,ths)\n",
    "    print(msg)\n",
    "    save_path = f\"./output/review_only_lstm_e{e}_b{BATCH_SIZE}_lr{LR}_l2{L2}_ber{BER:.4f}\"\n",
    "    torch.save(model.state_dict(),save_path)\n",
    "    log_text_to_file(f\"    ber after epoch {e+1}: {BER}, model params saved to {save_path}, other performance infos:{accu,TP,FP,TN,FN,TPR, FPR, TNR, FNR, BER}\")\n",
    "    log_text_to_file(msg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
